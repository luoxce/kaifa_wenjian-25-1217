# 本地Qwen模型快速开始

## 前提条件

你已经通过llamafactory下载了Qwen模型。现在需要将其部署为服务。

## 推荐方式：使用Ollama（最简单）

### 1. 安装Ollama

访问 https://ollama.ai 下载并安装。

### 2. 导入你的Qwen模型

如果你已经有Qwen模型文件，可以通过以下方式导入到Ollama：

```bash
# 方式1：如果模型已经在Ollama格式
ollama import /path/to/your/qwen/model

# 方式2：直接从llamafactory导出的模型转换
# 需要先将模型转换为Ollama格式（可能需要额外工具）
```

或者直接使用Ollama下载（如果网络允许）：

```bash
ollama pull qwen
```

### 3. 验证模型

```bash
# 列出已安装的模型
ollama list

# 测试模型
ollama run qwen "Hello, how are you?"
```

### 4. 配置回测系统

编辑 `backtest_config.yaml`：

```yaml
strategy:
  use_llm: true
  llm_strategy: "balanced"
  llm_provider: "local_qwen_ollama"
  llm_model_name: "qwen"  # 你的模型名称
```

### 5. 运行回测

```bash
python run_backtest.py --config backtest_config.yaml
```

或者在Web界面中选择"本地Qwen (Ollama)"。

## 如果Ollama不适用

### 使用VLLM（适合生产环境）

如果你有GPU且需要更好的性能：

```bash
# 安装vllm
pip install vllm

# 启动服务（假设你的模型在/path/to/qwen）
python -m vllm.entrypoints.openai.api_server \
    --model /path/to/your/qwen/model \
    --port 8000 \
    --trust-remote-code
```

然后在配置中使用：

```yaml
strategy:
  llm_provider: "local_qwen_vllm"
  llm_model_name: "qwen"
  llm_api_base: "http://localhost:8000"
```

## 常见问题

### Q: 我的模型是通过llamafactory下载的，如何导入Ollama？

A: llamafactory通常导出为HuggingFace格式。你需要：
1. 使用Ollama的`modelfile`功能
2. 或者使用转换工具（如`llama.cpp`）转换为Ollama格式
3. 或者直接使用VLLM，它支持HuggingFace格式

### Q: 如何知道我的模型名称？

A: 在Ollama中运行 `ollama list` 查看。如果是VLLM，使用模型路径或配置的名称。

### Q: 模型响应很慢怎么办？

A: 
- 检查是否有GPU加速
- 考虑使用量化版本（更小的模型）
- 使用VLLM而不是Ollama（性能更好）

### Q: 如何测试连接？

建议先用项目自带的自检脚本：

```bash
# Windows 可用 py，其它系统用 python
py check_llm.py --provider local_qwen_ollama --model-name qwen
```
